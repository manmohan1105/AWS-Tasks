# Convert CSV / JSON files to Apache Parquet using AWS Glue

### AWS Glue is fully managed and serverless ETL service from AWS


![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/csv-parquet-archtecture.PNG)

![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/awsglue.PNG)

### Why Parquet?
Parquet is a columnar file format and provides efficient storage. Better compression for columnar and encoding algorithms are in place. Mostly we are using the large files in Athena. BigQuery is also supported the Parquet file format. So we can have a better control in Performance and the Cost.

## Step 1 Create a s3 bucket for storing our files both parquet and csv
## Step 2 Create a Crawler 
* Create the crawlers:
* We need to create and run the Crawlers to identify the schema of the CSV files.
* Go to AWS Glue home page.
* From the Crawlers → add crawler.
* Give a name for you crawler.
* Data source S3 and the Include path should be you CSV files folder.
* Now let's point the crawler to your data. Choose the Amazon S3 data store and the option to crawl data in a Specified path in another account. Then for the Include path, type where the crawler can find the flights data, s3://crawler-public-us-east-2/flight/2016/csv.

![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/adddatastore-csv-path.PNG)


* The next step will ask to add more data source, Just click NO.
* Next one for selecting the IAM role. The crawlers needs read access of the S3, but save the Parquet files, it needs the Write access too. So create a role along with the following policies.
* `AWSGlueServiceRole ,
S3 Read/Write access for your bucket.`
* In the next step just let the crawler as Run as On Demand.
* Then it’ll ask a database name to create a table schema for the CSV file.  (*Crawlers create tables in your data catalog. Tables are contained in a database. Let's choose or create the flights-db database to contain the tables. If the database doesn't exist, choose Add database to create one.Enter flights for Prefix added to tables.Choose Next.*)

![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/database-glue.PNG)


* Run the Crawler Once its created, it’ll ask to run. Click run and wait for few mins, then you can see that it’s created a new table with the same schema of your CSV files in the Data catalogue.
![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/Crawler%20created.PNG)

Then it’ll create the table name as the CSV file location. [csv_file]

![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/crawler-table-created.PNG)

## Step 3 Create Parquet conversion Job:
Next, you want to convert the CSV files to Parquet format, and drop fields that you do not require your data analysis.
Let’s create an ETL job to extract, transform, and load relevant columns from your flights data from an Amazon S3 source to an Amazon S3 target.
* In the ETL Section, go to Jobs → add Job.
* Give a name for your job and select the IAM role(select the one which we have created in the previous step).
* for this tutorial First, name your job Flights Conversion. Then choose the IAM role your job assumes when it runs.Let AWS Glue generate the script by choosing "A proposed script generated by AWS Glue" and typing your Script file name, tutorial-script, and S3 path where the script is stored. Choose a Temporary directory in S3 also.Choose Next.

![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/job-configuration-glue.PNG)

* Choose Data Source: Select the datasource which is created by the crawler.
* In Choose your data target,Choose a data target Your job can create tables or update existing tables. In this tutorial, choose Create tables in your data target. Then choose Data store as S3 and Format as Parquet. Finally, choose an S3 location for the transformed data. Choose Next.

![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/data%20Target.PNG)

* The next windows is for column mapping. If you need to remap any column or remove any columns from CSV, you can achieve it from here.
* Now the next one will show you the Diagram and source code for the job. Just click the run job button.


![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/jobcreated.PNG)

* Wait for few mins(its based on your total amount of data) to complete the job. You can see the logs from the bottom.

* Lets check the files in S3. Go to s3  and see the converted files and their size.


![alt text](https://github.com/manmohan1105/AWS-Tasks-asssigned-by-Mentor-/blob/main/New%20folder/parquet%20created.PNG)



